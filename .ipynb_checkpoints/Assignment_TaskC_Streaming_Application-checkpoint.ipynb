{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUo5QwrHuzSI"
   },
   "source": [
    "# FIT5148 - Big data management and processing assignment\n",
    "\n",
    "# Assignment Task C Streaming Application#\n",
    "\n",
    "#### Team Members:\n",
    "\n",
    "1. 27033074 - Matthew Yeow Yit Hang\n",
    "\n",
    "2. 26546736 - Borris Trendy Wiria\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise client and drop collection for no duplication\n",
    "client = MongoClient()\n",
    "db = client.fit5148_assignment_db\n",
    "db = db.drop_collection(db.streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic\n",
    "topic = \"climate_hotspot\"\n",
    "\n",
    "# sender_id\n",
    "climate1 = \"climate_1\"\n",
    "hotspotAqua = \"hotspot_1\"\n",
    "hotspotTerra = \"hotspot_2\"\n",
    "\n",
    "# config data\n",
    "batch_interval = 10\n",
    "appName = \"StreamingApplication\"\n",
    "threads = \"local[2]\"\n",
    "bootstrap_servers = '127.0.0.1:9092'\n",
    "\n",
    "# config spark and spark streaming\n",
    "config = SparkConf().setAppName(appName).setMaster(threads)\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=config)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, batch_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create Dstreams\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "            'bootstrap.servers': bootstrap_servers\n",
    "        })\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "    hotspotList = []\n",
    "    climateList = []\n",
    "    \n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    streaming = db.streaming\n",
    "    \n",
    "    for record in iter:\n",
    "        data = record[1].split(',')\n",
    "        if data[-1] == climate1: # climate data\n",
    "            jsonData = {\n",
    "                \"geo_hash\": record[0],\n",
    "                \"latitude\": float(data[0]),\n",
    "                \"longitude\": float(data[1]),\n",
    "                \"air_temperature_celcius\": int(data[2]),\n",
    "                \"relative_humidity\": float(data[3]),\n",
    "                \"windspeed_knots\": float(data[4]),\n",
    "                \"max_wind_speed\": float(data[5]),\n",
    "                \"precipitation\": data[6],\n",
    "                \"hotspot\": [],\n",
    "                \"average_confidence\": 0,\n",
    "                \"average_surface_temperature\": 0\n",
    "            }\n",
    "            climateList.append(jsonData)\n",
    "        else:  # hotspot data\n",
    "            jsonData = {\n",
    "                \"geo_hash\": record[0],\n",
    "                \"latitude\": float(data[0]),\n",
    "                \"longitude\": float(data[1]),\n",
    "                \"confidence\": int(data[2]),\n",
    "                \"surface_temperature_celcius\": int(data[3]),\n",
    "                \"arrival_time\": data[-2]\n",
    "            }\n",
    "            hotspotList.append(jsonData)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    EXPLANATION FOR THE FOLLOWING CODE:\n",
    "    \n",
    "    1. For every climate in batch, we check for every hotspot that has the same geohash.\n",
    "    2. If there are corresponding geohash values, we increase the sum of confidence and temperature and append the hotspot document into the climate document.\n",
    "    3. The average is calculated if there are hotspots.\n",
    "    '''\n",
    "    for climate in climateList:\n",
    "        \n",
    "        sumConfidence = 0\n",
    "        sumSurfaceTemperature = 0\n",
    "        \n",
    "        \n",
    "        for hotspot in hotspotList:\n",
    "            # Join based on geohash\n",
    "            if climate[\"geo_hash\"] == hotspot[\"geo_hash\"]:  \n",
    "                sumConfidence += hotspot[\"confidence\"]\n",
    "                sumSurfaceTemperature += hotspot[\"surface_temperature_celcius\"]\n",
    "                climate[\"hotspot\"].append(hotspot)\n",
    "                \n",
    "        \n",
    "        if len(climate[\"hotspot\"]) != 0:\n",
    "            # Finding average\n",
    "            climate[\"average_confidence\"] = sumConfidence / len(climate[\"hotspot\"])\n",
    "            climate[\"average_surface_temperature\"] = sumSurfaceTemperature / len(climate[\"hotspot\"])\n",
    "            \n",
    "        try:\n",
    "            streaming.insert_one(climate)\n",
    "            #streaming.replace_one({\"_id\":climate[\"_id\"]}, climate, True)\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "    \n",
    "lines = kafkaStream.foreachRDD(lambda rdd:rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fjgEvZncuzTA",
    "7O2FIFXguzTG",
    "G600-wcVuzTM",
    "hdL3uMpmuzTa",
    "qcfxpKYUuzTf",
    "FVLhRZaKuzTm",
    "2KMqjsSTuzTr",
    "7Lgvt2pNuzT3",
    "YlbblI1SuzT8",
    "8XAuMPhguzUG"
   ],
   "name": "FIT5148 - MongoDB with Python.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
